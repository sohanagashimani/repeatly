# Repeatly - Self-Hosted Cron-as-a-Service

> Schedule and run HTTP jobs on your own infra. TypeScript, PostgreSQL, Redis, Docker.

---

## Architecture

```mermaid
graph TB
    %% User and Frontend Layer
    subgraph "User Interface Layer"
        USER["üë§ User<br/>Multi-timezone<br/>Dashboard"]
        FRONTEND["üé® React Frontend<br/>‚Ä¢ Job Management<br/>‚Ä¢ Execution Tracking<br/>‚Ä¢ Timezone Toggle<br/>‚Ä¢ API Key Management<br/>Deployed: Firebase Hosting"]
    end

    %% Authentication & Gateway Layer
    subgraph "Authentication & Gateway"
        FIREBASE["üîê Firebase Auth<br/>‚Ä¢ JWT Tokens<br/>‚Ä¢ User Management<br/>‚Ä¢ OAuth Integration"]
        GATEWAY["üåê Google Cloud<br/>API Gateway<br/>‚Ä¢ Rate Limiting<br/>‚Ä¢ JWT Validation<br/>‚Ä¢ Request Routing<br/>‚Ä¢ Cron Management (WIP)"]
    end

    %% Core Services Layer
    subgraph "Core Services - Cloud Run"
        API["‚ö° Express API<br/>packages/api<br/>‚Ä¢ Job CRUD<br/>‚Ä¢ JobSchedulingService<br/>‚Ä¢ Validation & Auth<br/>Deployed: Cloud Run"]
        SCHEDULER["üïê Partition Scheduler<br/>packages/scheduler<br/>‚Ä¢ 60s Polling Cycle<br/>‚Ä¢ Partition Pruning<br/>‚Ä¢ Atomic Job Claiming<br/>‚Ä¢ Batch Processing<br/>Deployed: Cloud Run"]
        WORKER["üöÄ Smart Worker<br/>packages/worker<br/>‚Ä¢ HTTP Execution<br/>‚Ä¢ Response Sanitization<br/>‚Ä¢ JobExecutionService<br/>‚Ä¢ Metadata Storage<br/>‚Ä¢ Schedules Next Runs<br/>Deployed: Cloud Run"]
    end

    %% Database Layer with Partitioning
    subgraph "Database Layer - PostgreSQL"
        MAINDB["üìä Main Tables<br/>‚Ä¢ Job<br/>‚Ä¢ User<br/>‚Ä¢ ApiKey"]

        subgraph "Hourly Partitioned Tables"
            PART00["‚è∞ scheduled_jobs_00<br/>Hour 0: 00:00-00:59"]
            PART01["‚è∞ scheduled_jobs_01<br/>Hour 1: 01:00-01:59"]
            PART02["‚è∞ scheduled_jobs_02<br/>Hour 2: 02:00-02:59"]
            PARTDOTS["‚ãÆ<br/>..."]
            PART23["‚è∞ scheduled_jobs_23<br/>Hour 23: 23:00-23:59"]
        end

        EXECDB["üìà Job Executions<br/>‚Ä¢ Execution History<br/>‚Ä¢ Performance Metrics<br/>‚Ä¢ Sanitized Responses"]
    end

    %% External Systems
    subgraph "External Systems"
        EXT["üåç Target APIs<br/>‚Ä¢ HTTP/HTTPS endpoints<br/>‚Ä¢ Webhooks<br/>‚Ä¢ REST APIs"]
    end

    %% Data Flow
    USER --> FRONTEND
    FRONTEND --> GATEWAY
    GATEWAY --> API
    API --> MAINDB
    API --> SCHEDULER
    SCHEDULER --> PART00
    SCHEDULER --> PART01
    SCHEDULER --> PART02
    SCHEDULER --> PARTDOTS
    SCHEDULER --> PART23
    SCHEDULER --> WORKER
    WORKER --> EXECDB
    WORKER --> EXT
    EXT --> WORKER
    WORKER --> PART00
    WORKER --> PART01
    WORKER --> PART02
    WORKER --> PARTDOTS
    WORKER --> PART23
    WORKER -- "Schedules Next Runs, Updates Status" --> PART00
    WORKER -- "Schedules Next Runs, Updates Status" --> PART01
    WORKER -- "Schedules Next Runs, Updates Status" --> PART02
    WORKER -- "Schedules Next Runs, Updates Status" --> PARTDOTS
    WORKER -- "Schedules Next Runs, Updates Status" --> PART23
```

---

## Project Structure

```
repeatly/
‚îú‚îÄ‚îÄ packages/
‚îÇ   ‚îú‚îÄ‚îÄ api/         # Express.js API server
‚îÇ   ‚îú‚îÄ‚îÄ scheduler/   # Partition-aware scheduler
‚îÇ   ‚îú‚îÄ‚îÄ worker/      # Job execution worker
‚îÇ   ‚îú‚îÄ‚îÄ database/    # Prisma schema, migrations, DB utils
‚îÇ   ‚îî‚îÄ‚îÄ frontend/    # React dashboard
```

---

## Quick Start

### Prerequisites

- Node.js 18+
- Docker & Docker Compose

### 1. Clone & Install

```bash
# Clone
git clone https://github.com/your-org/repeatly.git
cd repeatly

# Install deps
npm install
```

### 2. Local Dev (Manual)

```bash
# Start DB/Redis
npm run infra

# Run DB migrations
npm run db:migrate:deploy

# Start services (in separate terminals)
npm run dev:api
npm run dev:scheduler
npm run dev:worker
npm run dev:frontend
```

### 3. Docker Compose (Recommended)

Before running Docker Compose, copy `.env.example` at the project root to `.env` and fill in the required values:

```bash
cp .env.example .env
# Edit .env and set your secrets/credentials
```

Then start all services:

```bash
docker-compose up -d
```

---

## Environment Variables

Copy `.env.example` from `packages/database` and fill in values as needed:

```
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/repeatly
REDIS_URL=redis://default:devpassword123@redis:6379
```

### API Service (`packages/api/.env`)

| Variable                       | Description                                              |
| ------------------------------ | -------------------------------------------------------- |
| GOOGLE_APPLICATION_CREDENTIALS | Base64-encoded Google service account credentials (JSON) |
| PORT                           | Port for the API server (default: 3000)                  |
| NODE_ENV                       | Node environment (e.g., development, production)         |
| # API_GATEWAY_HOST             | (Optional) API Gateway host for API key management       |

- `GOOGLE_APPLICATION_CREDENTIALS` should be a base64-encoded JSON key for a Google Cloud service account. This is used for authentication and integration with Google Cloud services.
- `PORT` and `NODE_ENV` are standard Node.js environment variables.
- `API_GATEWAY_HOST` is optional and only needed if using API Gateway for key management.

### Frontend (`packages/frontend/.env`)

| Variable                  | Description                                        |
| ------------------------- | -------------------------------------------------- |
| VITE_FIREBASE_API_KEY     | Firebase project API key                           |
| VITE_FIREBASE_AUTH_DOMAIN | Firebase Auth domain (e.g., xxx.firebaseapp.com)   |
| VITE_FIREBASE_PROJECT_ID  | Firebase project ID                                |
| VITE_API_BASE_URL         | Base URL for the API (e.g., http://localhost:3000) |

- All variables are required for the frontend to connect to Firebase Auth and the backend API.
- `VITE_API_BASE_URL` should point to your running API instance.

## What's Next

- **API-based Cron Management (API Gateway, WIP):**
  - Manage the full lifecycle of cron jobs including creation, updates, deletion, and viewing execution history via REST APIs through the API Gateway (coming soon).
- **UI/UX Improvements:**
  - Enhanced dashboard maybe?
- **Integrations:**
  - Support for more notification channels (Slack, email, webhooks).
- **Observability:**
  - Improved metrics, logs, and alerting for job executions.
- **Self-service Onboarding:**
  - Better documentation, onboarding flows, and example jobs.
- **Infra:**
  - Switch to a self-hosted Redis database for production reliability.
  - Manage all services in Kubernetes for advanced orchestration and scaling.
- **Security Enhancements:**
  - Audit logs, and SSO options.
- **Community Contributions:**
  - Open to PRs for new features, bug fixes, and documentation improvements.

---

## Deployment & CI/CD

- **Backend (API, Scheduler, Worker):**

  - Deployed to Google Cloud Run using Docker images.
  - Automated via GitHub Actions workflow: `.github/workflows/deploy-all-services.yml`.
  - Handles build, push, and deploy for all backend services, including database migrations.

- **Frontend:**

  - Deployed to Firebase Hosting.
  - Automated via GitHub Actions workflow: `.github/workflows/deploy-frontend.yml`.
  - Handles build and deploy of the frontend.

- **How it works:**
  - On workflow dispatch, the CI builds Docker images, pushes to Google Artifact Registry, and deploys to Cloud Run (backend) or Firebase Hosting (frontend).
  - Secrets and environment variables are managed via GitHub and Google Secret Manager.

See the respective workflow files for more details and customization options.
